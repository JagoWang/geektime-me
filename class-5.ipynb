{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "\n",
    "from openai.embeddings_utils import get_embedding, get_embeddings\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "# import data/toutiao_cat_data.txt as a pandas dataframe\n",
    "df = pd.read_csv('/Users/indigo/Documents/personalspace/geektime-me/toutiao_cat_data.txt', sep='_!_', names=['id', 'code', 'category', 'title', 'keywords'])\n",
    "df = df.fillna(\"\")\n",
    "df[\"combined\"] = (\n",
    "    \"标题: \" + df.title.str.strip() + \"; 关键字: \" + df.keywords.str.strip()\n",
    ")\n",
    "\n",
    "print(\"Lines of text before filtering: \", len(df))\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "# omit reviews that are too long to embed\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
    "df = df[df.n_tokens <= max_tokens]\n",
    "\n",
    "print(\"Lines of text after filtering: \", len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 下面代码不能跑，串行，几个小时跑不完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def get_embedding_with_backoff(**kwargs):\n",
    "    return get_embedding(**kwargs)\n",
    "\n",
    "# randomly sample 10k rows\n",
    "df_10k = df.sample(10000, random_state=42)\n",
    "\n",
    "df_10k[\"embedding\"] = df_10k.combined.apply(lambda x : get_embedding_with_backoff(text=x, engine=embedding_model))\n",
    "df_10k.to_csv(\"/Users/indigo/Documents/personalspace/geektime-me/toutiao_cat_data_10k_with_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 下面代码并行，也得半个小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import backoff\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def get_embeddings_with_backoff(prompts, engine):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        embeddings += get_embeddings(list_of_text=batch, engine=engine)\n",
    "    return embeddings\n",
    "\n",
    "# randomly sample 10k rows\n",
    "df_all = df\n",
    "# group prompts into batches of 100\n",
    "prompts = df_all.combined.tolist()\n",
    "prompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n",
    "\n",
    "embeddings = []\n",
    "for batch in prompt_batches:\n",
    "    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n",
    "    embeddings += batch_embeddings\n",
    "\n",
    "df_all[\"embedding\"] = embeddings\n",
    "df_all.to_parquet(\"/Users/indigo/Documents/personalspace/geektime-me/toutiao_cat_data_all_with_embeddings.parquet\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 下面代码直接用处理好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "training_data = pd.read_parquet(\"/Users/indigo/Documents/personalspace/geektime-ai-course/data/20_newsgroup_with_embedding.parquet\")\n",
    "training_data.head()\n",
    "\n",
    "df =  training_data.sample(50000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values), df.category, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=300)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df =  training_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    list(df.embedding.values), df.category, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
